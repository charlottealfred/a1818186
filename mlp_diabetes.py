# -*- coding: utf-8 -*-
"""MLP diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ioI_O_8sfAwcIUTDKxDfQqh9SumVRH_

# Load data
"""

# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"
from IPython.display import clear_output
from tqdm import trange
import pandas as pd
from tensorflow import keras
import numpy as np ## For numerical python
np.random.seed(42)

import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
# plt.rc('figure', dpi=120) # set good resolution

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_svmlight_file


from google.colab import drive
drive.mount('/content/drive')

#read libsvm file and transform to numpy array
df, label = load_svmlight_file('/content/drive/My Drive/ass3/diabetes.txt')
label[label < 0] = 0
label = label.astype(int)
df = df.toarray()

df1 = pd.DataFrame(df)
df1.describe()

# check NAN value
print([i for i in np.sum(df1.isna())])

label1 = pd.DataFrame(label)
print(label1.describe())

# check NAN value
print([i for i in np.sum(label1.isna())])

label1.plot(kind = 'hist',title ='The proportion of two class')

"""## split df into train set (70%), validation set (15%) and test set (15%)

"""

X_train,X_,y_train, y_ =sklearn.model_selection.train_test_split\
    (df,label,test_size=0.3, random_state=0,stratify=label)
X_test,X_val,y_test, y_val =sklearn.model_selection.train_test_split\
    (X_,y_,test_size=0.5, random_state=0,stratify=y_)

pd.DataFrame(y_train).plot(kind = 'hist',title ='The proportion of two class in train set')
plt.show()
pd.DataFrame(y_val).plot(kind = 'hist',title ='The proportion of two class in validation set')
plt.show()
pd.DataFrame(y_test).plot(kind = 'hist',title ='The proportion of two class in test set')
plt.show()

"""# Building network

## full connected layer
$\frac{\partial H(PO)}{\partial PO} = q_i - p_i$

$\frac{\partial PO}{\partial f(s)} = W_{2}$

$\frac{\partial f(s)}{\partial s} = f(s)'$

$\frac{\partial s}{\partial W_{1}} = X$

$\frac{\partial H(PO)}{\partial W_{1}}=(q_i - p_i)*W_{2}*f(s)'*X$


update $W_{1}$

$W_{1new}=W_{1old}-\eta *\frac{\partial H(PO)}{\partial W_{1}}$
"""

class Layer:

  
    def __init__(self):
        
        pass

    def forward(self, input):
        
        return input

    def backward(self, input, grad_output):
        
        num_units = input.shape[1]

        d = np.eye(num_units)

        return np.dot(grad_output, d)  # chain rule


class Dense(Layer):
    def __init__(self, input_units, output_units, learning_rate=0.1):
        # f(x) = <W*x> + b

        self.learning_rate = learning_rate
        self.weights = np.random.normal(loc=0.0,
                                        scale=np.sqrt(2 / (input_units + output_units)),
                                        size=(input_units, output_units))
        self.biases = np.zeros(output_units)

    def forward(self, input):
        # f(x) = <W*x> + b

        # input shape: [batch, input_units]
        # output shape: [batch, output units]

        return np.dot(input, self.weights) + self.biases

    def backward(self, input, grad_output):
        '''grad_add_up is the cumulative product of the 
         gradient of the loss function to the gradient of this layer， 
         which is used for the next round of backpropagation
         $\frac{\partial H(PO)}{\partial W_{1}}=(q_i - p_i)*W_{2}*f(s)'*X$'''
        grad_add_up = np.dot(grad_output, self.weights.T)

        # compute gradient w.r.t. weights and biases
        # $\frac{\partial s}{\partial W_{1}} = X$
        grad_weights = np.dot(input.T, grad_output)
        grad_biases = grad_output.mean(axis=0) * input.shape[0]

        # $W_{1new}=W_{1old}-\eta *\frac{\partial H(PO)}{\partial W_{1}}$
        self.weights = self.weights - self.learning_rate * grad_weights
        self.biases = self.biases - self.learning_rate * grad_biases

        return grad_add_up

"""##activation function"""

class ReLU(Layer):
    def __init__(self):
        pass

    def forward(self, input):
        relu_forward = np.maximum(0, input)
        return relu_forward

    def backward(self, input, grad_output):
        relu_grad = input > 0
        return grad_output * relu_grad

class Sigmoid(Layer):
    def __init__(self):
        pass

    def forward(self, input):
        return 1./(1.+np.exp(input))
        # return relu_forward

    def backward(self, input,grad_output):
        s = 1 / (1 + np.exp(-input))
        ds = s * (1 - s)
        return grad_output*ds


class Tanh(Layer):
    def __init__(self):
        pass

    def forward(self, input):
        return np.tanh(input)

    def backward(self, input,grad_output):
        ds =  1/(np.cosh(input)**2)
        # print(output,output.shape)
        return grad_output*ds

"""## Loss function
$H$ is the cross-entropy value: 
$H(P,Q)=-\sum_{x}^{} P(x)Log(Q(x))$


The cross-entropy describes the distance between two probability distribution vectors, the formula P is the probability distribution vector of the true value, Q is the probability distribution vector of the predicted value of the network, assuming that the feedforward neural network is used to solve the three classification problems, Q is [0.1,0.1,0.8], the probability of the largest is the true answer, P is [0,0,1]
$H= 0*log(0.1)+0*log(0.1)+1*log(0.8)$
Cross entropy is generally used with softmax regression, softtmax regression converts the output value of the neural network into a probability distribution vector.

The softmax function transforms a vector of K real values into a vector with a sum of K real values of 1. The input values can be positive, negative, zero, or greater than 1, but softmax transforms them into values between 0 and 1 so that they can be interpreted as probabilities.
$q_i = \frac{e^{q_i}}{\sum_{k=1}^N e^q_k}$


Derivative of Cross Entropy Loss with Softmax

$\frac{\partial H(P,Q)}{\partial o_i} = q_i - p_i$


"""

def softmax_crossentropy_with_logits(predict_answer, right_answers):
    prob_right_answers = predict_answer[np.arange(len(predict_answer)), right_answers]

    entropy = - prob_right_answers + np.log(np.sum(np.exp(predict_answer), axis=-1))

    return entropy

def grad_softmax_crossentropy_with_logits(logits, right_answers):
    right_answers_one_hot = np.zeros_like(logits) #Initialize matrix
    # because P(x) is [0,1] or [1,0], 1 means the right answer ,only select the answer with right answer and one hot encode it  
    right_answers_one_hot[np.arange(len(logits)), right_answers] = 1

    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True) #$q_i = \frac{e^{q_i}}{\sum_{k=1}^N e^q_k}$


    
    return (- right_answers_one_hot + softmax) / logits.shape[0]# $\frac{\partial H(P,Q)}{\partial o_i} = q_i - p_i$

"""## forward propagation and backword propagation"""

def forward_propagation(network, X):
   

    activations = []
    input = X
    # Looping through each layer
    for l in network:
        activations.append(l.forward(input))
        # Updating input to last layer output
        input = activations[-1]

    assert len(activations) == len(network)
    return activations


def predict(network, X):
    logits = forward_propagation(network, X)[-1]
    return logits.argmax(axis=-1)


def train(network, X, y):
   
    layer_activations = forward_propagation(network, X)
    layer_inputs = [X] + layer_activations  
    ''' layer_input[i] is an input for network[i], 
     add input X as the first for calculating the gradient'''
    logits = layer_activations[-1] #output of the whole network

    loss = softmax_crossentropy_with_logits(logits, y)
    loss_grad = grad_softmax_crossentropy_with_logits(logits, y)

    # backword propagation
    for layer_index in range(len(network))[::-1]:
        # print(layer_index)
        layer = network[layer_index]
        # print(loss_grad,loss_grad.shape)
        loss_grad = layer.backward(layer_inputs[layer_index], loss_grad)  # grad w.r.t. input, also weight updates

    return np.mean(loss)




def get_minibatches(inputs, targets, batchsize):
   
    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):
        
        excerpt = slice(start_idx, start_idx + batchsize)
        yield inputs[excerpt], targets[excerpt]

"""# Train the network"""

# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"
from IPython.display import clear_output
from tqdm import trange


from tensorflow import keras

import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
import seaborn as sns; sns.set()
# plt.rc('figure', dpi=120) # set good resolution

from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.datasets import load_svmlight_file


from google.colab import drive
drive.mount('/content/drive')


df, label = load_svmlight_file('/content/drive/My Drive/ass3/diabetes.txt')
label[label < 0] = 0
label = label.astype(int)
df = df.toarray()
X_train,X_,y_train, y_ =sklearn.model_selection.train_test_split\
    (df,label,test_size=0.3, random_state=0,stratify=label)
X_test,X_val,y_test, y_val =sklearn.model_selection.train_test_split\
    (X_,y_,test_size=0.5, random_state=0,stratify=y_)

import numpy as np ## For numerical python
np.random.seed(42)


class Layer:

  
    def __init__(self):
        
        pass

    def forward(self, input):
        
        return input

    def backward(self, input, grad_output):
        
        num_units = input.shape[1]

        d = np.eye(num_units)

        return np.dot(grad_output, d)  # chain rule

class ReLU(Layer):
    def __init__(self):
        pass

    def forward(self, input):
        relu_forward = np.maximum(0, input)
        return relu_forward

    def backward(self, input, grad_output):
        relu_grad = input > 0
        return grad_output * relu_grad

class Sigmoid(Layer):
    def __init__(self):
        pass

    def forward(self, input):
        return 1./(1.+np.exp(input))
        # return relu_forward

    def backward(self, input,grad_output):
        s = 1 / (1 + np.exp(-input))
        ds = s * (1 - s)
        return grad_output*ds


class Tanh(Layer):
    def __init__(self):
        pass

    def forward(self, input):
        return np.tanh(input)

    def backward(self, input,grad_output):
        ds =  1/(np.cosh(input)**2)
        # print(output,output.shape)
        return grad_output*ds

class Dense(Layer):
    def __init__(self, input_units, output_units, learning_rate=0.1):
        # f(x) = <W*x> + b

        self.learning_rate = learning_rate
        self.weights = np.random.normal(loc=0.0,
                                        scale=np.sqrt(2 / (input_units + output_units)),
                                        size=(input_units, output_units))
        self.biases = np.zeros(output_units)

    def forward(self, input):
        # f(x) = <W*x> + b

        # input shape: [batch, input_units]
        # output shape: [batch, output units]

        return np.dot(input, self.weights) + self.biases

    def backward(self, input, grad_output):
        
        grad_add_up = np.dot(grad_output, self.weights.T)

        grad_weights = np.dot(input.T, grad_output)
        grad_biases = grad_output.mean(axis=0) * input.shape[0]


        self.weights = self.weights - self.learning_rate * grad_weights
        self.biases = self.biases - self.learning_rate * grad_biases

        return grad_add_up


def softmax_crossentropy_with_logits(predict_answer, right_answers):
    prob_right_answers = predict_answer[np.arange(len(predict_answer)), right_answers]

    entropy = - prob_right_answers + np.log(np.sum(np.exp(predict_answer), axis=-1))

    return entropy

def grad_softmax_crossentropy_with_logits(logits, right_answers):
    right_answers_one_hot = np.zeros_like(logits)
    right_answers_one_hot[np.arange(len(logits)), right_answers] = 1

    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)

    return (- right_answers_one_hot + softmax) / logits.shape[0]





def forward_propagation(network, X):
   

    activations = []
    input = X
    # Looping through each layer
    for l in network:
        activations.append(l.forward(input))
        # Updating input to last layer output
        input = activations[-1]

    assert len(activations) == len(network)
    return activations


def predict(network, X):
    logits = forward_propagation(network, X)[-1]
    return logits.argmax(axis=-1)


def train(network, X, y):
   
    layer_activations = forward_propagation(network, X)
    layer_inputs = [X] + layer_activations  # layer_input[i] is an input for network[i]
    logits = layer_activations[-1]

    loss = softmax_crossentropy_with_logits(logits, y)
    loss_grad = grad_softmax_crossentropy_with_logits(logits, y)

    
    for layer_index in range(len(network))[::-1]:
        # print(layer_index)
        layer = network[layer_index]
        # print(loss_grad,loss_grad.shape)
        loss_grad = layer.backward(layer_inputs[layer_index], loss_grad)  

    return np.mean(loss)




def get_minibatches(inputs, targets, batchsize):
   
    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):
        
        excerpt = slice(start_idx, start_idx + batchsize)
        yield inputs[excerpt], targets[excerpt]

"""## result of Sigmoid"""

summarySig= []
for lr in [0.001,0.01,0.1]:
    for bat in [32,64,128]:
        print(lr,bat)
        network = []
        network.append(Dense(X_train.shape[1], 100, lr))
        network.append(Sigmoid())
        network.append(Dense(100, 200))
        network.append(Sigmoid())
        network.append(Dense(200, 2))

        train_record = []
        val_record = []
        for epoch in range(1500):
          for x_batch, y_batch in get_minibatches(np.array(X_train), np.array(y_train).ravel(), batchsize=bat):
              train(network, x_batch, y_batch)

          train_record.append(np.mean(predict(network, np.array(X_train)) == np.array(y_train).ravel()))
          val_record.append(np.mean(predict(network, np.array(X_val)) == np.array(y_val).ravel()))

          clear_output()
          print("Epoch", epoch)
          print("Train accuracy:", train_record[-1])
          print("Val accuracy:", val_record[-1])
        summarySig.append([bat,lr,train_record[-1],val_record[-1]])
        plt.plot(train_record, label='train accuracy')
        plt.plot(val_record, label='val accuracy')
        plt.legend(loc='best')
        plt.grid()
        
        fname = [bat,lr,"sig.png"]
        fname = "".join([str(_) for _ in fname])
        plt.savefig(fname, dpi=300)
        plt.show()

summarySig

for lr in [0.001,0.01,0.1]:
    for bat in [32,64,128]:
      print("-----------------------------------------") 
      print('Learning rate:',lr,'batch size:',bat)
      fname = [bat,lr,"sig.png"]
      fname = "".join([str(_) for _ in fname])
      print(fname)
      image = plt.imread(fname)
      plt.grid()
      plt.imshow(image)
      plt.show()



"""## result of ReLU"""



summaryRELU= []
for lr in [0.001,0.01,0.1]:
    for bat in [32,64,128]:
        print(lr,bat)
        network = []
        network.append(Dense(X_train.shape[1], 100, lr))
        network.append(ReLU())
        network.append(Dense(100, 200))
        network.append(ReLU())
        network.append(Dense(200, 2))

        train_record = []
        val_record = []
        for epoch in range(1500):
          for x_batch, y_batch in get_minibatches(np.array(X_train), np.array(y_train).ravel(), batchsize=bat):
              train(network, x_batch, y_batch)

          train_record.append(np.mean(predict(network, np.array(X_train)) == np.array(y_train).ravel()))
          val_record.append(np.mean(predict(network, np.array(X_val)) == np.array(y_val).ravel()))

          clear_output()
          print("Epoch", epoch)
          print("Train accuracy:", train_record[-1])
          print("Val accuracy:", val_record[-1])
        summaryRELU.append([bat,lr,train_record[-1],val_record[-1]])
        plt.plot(train_record, label='train accuracy')
        plt.plot(val_record, label='val accuracy')
        plt.legend(loc='best')
        plt.grid()
        
        fname = [bat,lr,"RELU.png"]
        fname = "".join([str(_) for _ in fname])
        plt.savefig(fname, dpi=300)
        plt.show()

summaryRELU

for lr in [0.001,0.01,0.1]:
    for bat in [32,64,128]:
      print("-----------------------------------------") 
      print('Learning rate:',lr,'batch size:',bat)
      fname = [bat,lr,"RELU.png"]
      fname = "".join([str(_) for _ in fname])
      print(fname)
      image = plt.imread(fname)
      plt.grid()
      plt.imshow(image)
      plt.show()

"""## result of Tanh"""

summary32 =summary64 =summary128 = []
for lr in [0.001,0.01,0.1]:
    for bat in [32,64,128]:
        print(lr,bat)
        network = []
        network.append(Dense(X_train.shape[1], 100, lr))
        network.append(Tanh())
        network.append(Dense(100, 200))
        network.append(Tanh())
        network.append(Dense(200, 2))

        train_record = []
        val_record = []
        for epoch in range(1500):
          for x_batch, y_batch in get_minibatches(np.array(X_train), np.array(y_train).ravel(), batchsize=bat):
              train(network, x_batch, y_batch)

          train_record.append(np.mean(predict(network, np.array(X_train)) == np.array(y_train).ravel()))
          val_record.append(np.mean(predict(network, np.array(X_val)) == np.array(y_val).ravel()))

          clear_output()
          print("Epoch", epoch)
          print("Train accuracy:", train_record[-1])
          print("Val accuracy:", val_record[-1])
        summary32.append([bat,lr,train_record[-1],val_record[-1]])
        plt.plot(train_record, label='train accuracy')
        plt.plot(val_record, label='val accuracy')
        plt.legend(loc='best')
        plt.grid()
        
        fname = [bat,lr,".png"]
        fname = "".join([str(_) for _ in fname])
        plt.savefig(fname, dpi=300)
        plt.show()
        # if bat == 32:
        #     summary32.append([lr,train_record[-1],val_record[-1]]), bbox_inches='tight'
        # if bat == 64:
        #     summary64.append([lr,train_record[-1],val_record[-1]])
        # if bat == 128:
        #     summary128.append([lr,train_record[-1],val_record[-1]])
        # Epoch 24
        # Train accuracy: 1.0
        # Val accuracy: 0.9809summary32 =summary64 =summary128 = []

summary32

for lr in [0.001,0.01,0.1]:
    for bat in [32,64,128]:
      print("-----------------------------------------") 
      print('Learning rate:',lr,'batch size:',bat)
      fname = [bat,lr,".png"]
      fname = "".join([str(_) for _ in fname])
      print(fname)
      image = plt.imread(fname)
      plt.grid()
      plt.imshow(image)
      plt.show()

image = plt.imread("1280.1.png")
plt.grid()
plt.imshow(image)

"""## Network 1"""

summaryal_tanh = []
for lr in [0.1]:
    for bat in [128]:
        print(lr,bat)
        network = []
        network.append(Dense(X_train.shape[1], 100, lr))
        network.append(Tanh())
        network.append(Dense(100, 100))
        network.append(Tanh())
        network.append(Dense(100, 100))
        network.append(Tanh())
        network.append(Dense(100, 100))
        network.append(Tanh())
        network.append(Dense(100, 2))

        train_record = []
        val_record = []
        for epoch in range(1000):
          for x_batch, y_batch in get_minibatches(np.array(X_train), np.array(y_train).ravel(), batchsize=bat):
              train(network, x_batch, y_batch)

          train_record.append(np.mean(predict(network, np.array(X_train)) == np.array(y_train).ravel()))
          val_record.append(np.mean(predict(network, np.array(X_val)) == np.array(y_val).ravel()))

          clear_output()
          print("Epoch", epoch)
          print("Train accuracy:", train_record[-1])
          print("Val accuracy:", val_record[-1])
        summaryal_tanh.append([bat,lr,train_record[-1],val_record[-1]])
        plt.plot(train_record, label='train accuracy')
        plt.plot(val_record, label='val accuracy')
        plt.legend(loc='best')
        plt.grid()
        
        fname = [bat,lr,"al.png"]
        fname = "".join([str(_) for _ in fname])
        plt.savefig(fname, dpi=300)
        plt.show()

"""## Network 2"""

summary_tanh_an = []
for lr in [0.1]:
    for bat in [128]:#128
        print(lr,bat)
        network = []
        network.append(Dense(X_train.shape[1], 300, lr))
        network.append(Tanh())
        network.append(Dense(300, 300))
        network.append(Tanh())
        network.append(Dense(300, 2))

        train_record = []
        val_record = []
        for epoch in range(1500):#1000
          for x_batch, y_batch in get_minibatches(np.array(X_train), np.array(y_train).ravel(), batchsize=bat):
              train(network, x_batch, y_batch)

          train_record.append(np.mean(predict(network, np.array(X_train)) == np.array(y_train).ravel()))
          val_record.append(np.mean(predict(network, np.array(X_val)) == np.array(y_val).ravel()))

          clear_output()
          print("Epoch", epoch)
          print("Train accuracy:", train_record[-1])
          print("Val accuracy:", val_record[-1])
        summary_tanh_an.append([bat,lr,train_record[-1],val_record[-1]])
        plt.plot(train_record, label='train accuracy')
        plt.plot(val_record, label='val accuracy')
        plt.legend(loc='best')
        plt.grid()
        
        fname = [bat,lr,"an.png"]
        fname = "".join([str(_) for _ in fname])
        plt.savefig(fname, dpi=300)
        plt.show()

"""## Network 3"""

summary_tanh_500 = []
for lr in [0.1]:
    for bat in [128]:#128
        print(lr,bat)
        network = []
        network.append(Dense(X_train.shape[1], 500, lr))
        network.append(Tanh())
        network.append(Dense(500, 500))
        network.append(Tanh())
        network.append(Dense(500, 200))
        network.append(Tanh())
        network.append(Dense(200, 2))

        train_record = []
        val_record = []
        for epoch in range(1000):#1500 
          for x_batch, y_batch in get_minibatches(np.array(X_train), np.array(y_train).ravel(), batchsize=bat):
              train(network, x_batch, y_batch)

          train_record.append(np.mean(predict(network, np.array(X_train)) == np.array(y_train).ravel()))
          val_record.append(np.mean(predict(network, np.array(X_val)) == np.array(y_val).ravel()))

          clear_output()
          print("Epoch", epoch)
          print("Train accuracy:", train_record[-1])
          print("Val accuracy:", val_record[-1])
        summary_tanh_500.append([bat,lr,train_record[-1],val_record[-1]])
        plt.plot(train_record, label='train accuracy')
        plt.plot(val_record, label='val accuracy')
        plt.legend(loc='best')
        plt.grid()
        
        fname = [bat,lr,"500.png"]
        fname = "".join([str(_) for _ in fname])
        plt.savefig(fname, dpi=300)
        plt.show()

"""## Testing the network 3"""

# print("Testing accuracy is:")
result = predict(network, np.array(X_test))
result
# np.mean(result == np.array(y_train).ravel())

import seaborn as sn
print("The F1 score is:")
print(sklearn.metrics.f1_score(y_test,result))
cm = sklearn.metrics.confusion_matrix(y_test,result)
s = sn.heatmap(cm, annot=True)
s.set_xlabel('Predicted', fontsize=10)
s.set_ylabel('True', fontsize=10)
plt.show()